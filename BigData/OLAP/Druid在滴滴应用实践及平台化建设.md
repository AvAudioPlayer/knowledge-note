# OLAP 数据库引擎 Druid 在滴滴应用实践及平台化建设是怎样的？

[原文链接](https://mp.weixin.qq.com/s/ByDu4XPy-xbdtefgI6w5dQ)

**摘要：**

Druid 是一款支持数据实时写入、低延时、高性能的 OLAP 引擎，具有优秀的数据聚合能力与实时查询能力。在大数据分析、实时计算、监控等领域都有特定的应用场景，是大数据基础架构建设中重要的一环。Druid 在滴滴承接了包括实时报表、监控、数据分析、大盘展示等应用场景的大量业务，作为大数据基础设施服务于公司多条业务线。本次演讲我们将介绍 Druid 的核心特性与原理，以及在滴滴内部大规模使用中积累的经验。

**分享大纲：**

[一、Druid 特性简介](#一、Druid 特性简介)
2、Druid 在滴滴的应用
3、Druid 平台化建设
4、展望

# 一、Druid 特性简介

Druid 是针对时间序列数据提供的低延时数据写入以及快速交互式查询的分布式 OLAP 数据库。其两大关键点是：首先，Druid 主要针对时间序列数据提供低延时数据写入和快速聚合查询; 其次，Druid 是一款分布式 OLAP 引擎。
　　
针对第一个特点来看，Druid 与典型的 TSDB，比如 InfluxDB、Graphite、OpenTSDB 的部分特性类似。这些时序数据库具备一些共同特点，一是写入即可查，通过内存增量索引让数据写入便可查询; 二是下采样或 RDD，通过下采样或类似于 RDD 的操作减少数据量，而 Druid 在数据写入时就会对数据预聚合，进而减少原始数据量，节省存储空间并提升查询效率; 三是可能会支持 Schema less，在 InfluxDB 中，用户可任意增加 tag，InfluxDB 可对新增 tag 进行聚合查询，但 Druid 在这点上与 InfluxDB 略有差异，Druid 需要预先定义 Schema 。Druid 的 Schema 数据打包在最后形成的数据文件中，数据文件按照时间分片，也就是说过去和未来数据的 Schema 可以不同，而不同 schema 的数据可以共存。所以，虽然 Druid 不是 schema less 的，但是 Schema 调整也是比较灵活。
　　
另外，Druid 作为一个 OLAP 数据库。OLAP 数据库需要支持类似上卷、切块、切片、下钻等操作，但不适合明细查询。对于类似根据某主键 ID 定位唯一数据的任务，OLAP 数据库并不能友好支持。常用的 OLAP 数据库实现方式以下几种：1) 数据检索引擎，比如 ES;2) 预计算加 KV 存储实现，比如 Kylin;3)SQL on Hadoop 引擎，比如 Presto、SparkSQL。
　　
接下来，我们就以上中实现进行对比。首先是数据检索引擎的代表 ES，ES 可以存储结构化和非结构化数据，同时具备明细查询和聚合查询能力，由于其自身是一个数据检索引擎，其索引类型并不是针对聚合分析设计的，所以聚合查询方面开销较大; 其次，ES 不但要保存所有的原始数据，还需要生成较多的索引，所以存储空间开销会更大，数据的写入效率方面会比 Druid 差一些。
　　
与 ES 相比，Druid 只能处理结构化数据，因为它必须预定义 Schema; 其次，Druid 会对数据进行预聚合以减少存储空间，同时对数据写入和聚合进行优化。但是，由于进行了预聚合，所以 Druid 抛弃掉了原始数据，导致其缺少原始明细数据查询能力。如果业务方有需求，可以关闭预聚合，但会丧失 Druid 的优势。
　　
其次是预计算 + kv 存储方式 ，KV 存储需要通过预计算实现聚合，可以认为 Key 涵盖了查询参数，而值就是查询结果，由于直接从 KV 存储进行查询，所以速度非常快。缺点是因为需要在预计算中处理预设的聚合逻辑，所以损失了查询灵活性，复杂场景下的预计算过程可能会非常耗时，而且面临数据过于膨胀的情况; 由于只有前缀拼配一种索引方式，所以在大数据量的复杂过滤条件下，性能下降明显; 且缺少聚合下推能力。
　　
与预计算 + KV 存储方式相比，Druid 是使用 Bitmap 索引的列式存储，查询速度肯定不如 KV 存储快; 但是由于使用内存增量索引，增量预聚合的模式，写入即可查，无需等待预计算生成 Cube，所以实时性更强; 其次，Druid 可针对任意维度组合过滤、聚合，查询更加灵活; 最后，Scatter & Gather 模式支持一定的聚合下推。
　　
最后是 SQL on Hadoop， 这类引擎的 SQL 支持通常很强大，且无冗余数据，不需要预处理。缺点是因为其直接通过计算引擎对 Hadoop 上的文件进行操作，所以响应速度较慢且 QPS 相对较低。
　　
与 SQL on Hadoop 方式相比，Druid 的 SQL 支持有限，但在逐渐完善; 必须预定义维度指标。其优势在于可达到亚秒级响应，并发较高。

# 二、Durid 在滴滴的应用
Druid 目前在滴滴使用规模大概为多个集群百余台机器，日原始数据写入量在千亿级别，日落盘数据在 TB 级别，数百实时数据源、千级实时写入任务，日查询量近千万级。主要承接业务有监控、实时报表，大屏展示等。
　　
下图为滴滴实时业务监控案例：
![](/assets/Durid 在滴滴的应用_图2-1.jpg)

我们的监控体系大概可以分为三层：顶层为业务监控，主要由业务方定义指标，然后配置相应的查询和报警。主要目的在于及时发现业务问题并告警; 中层的监控体系是对各服务网关调用的监控日志，主要为了发现某业务问题造成的影响范围和具体影响对象; 底层运维体系主要对网络、机器各方面指标进行监控。
　　
之所以业务监控适用 Druid，是因为业务指标通常具有较为复杂多变的业务逻辑。Druid 本身是一个 OLAP 引擎，定义一个数据源就可衍生出众多聚合指标，所以很适合这类灵活查询的配置。
　　
第二类应用是实时报表类应用 (如下图)，实时报表类应用主要用于运营数据分析，客户端网络性能分析以及客服应答实时统计等。这些用户通常是从 Hive 数据仓库迁移过来的，因为希望获得实时用户体验而 Hive 查询速度太慢，所以选择迁移。典型应用场景比如快速获取某下雨区域的用户单据，对用户进行优惠券投放进而刺激用户打车。
![](/assets/Durid 在滴滴的应用_图2-2.jpg)

第三类是大屏展示类应用 (如下图)，这类应用主要用于呈现业务性关键结果，通常是 PV、UV 或 TOP N 查询，非常适合 Druid。
![](/assets/Durid 在滴滴的应用_图2-3.jpg)

# 三、Druid 平台化建设

在承接应用场景的过程中，我们做了很多平台化建设。简单啊介绍下平台化建设的背景： 业务数据主要来源是日志和 binlog; 公司统一数据通道是 kafka; 业务指标多样，逻辑复杂多变; Druid 接入配置较复杂，除 Schema 配置外，还包括实时任务配置; 数据进入 Druid 之前通常需要流计算处理，业务方自己开发既费时又很容易出现问题; Druid 数据的对应关系以及数据源衍生指标链路较长，需要进行上下游关系梳理; 由于 Druid 官方主要通过 API 查询，未提供数据可视化服务组件，因此业务方急需数据可视化相关服务组件。
　　
在以上的背景下，我们构建了实时计算平台，架构图如下：
![](/assets/Druid 在滴滴应用_图3-1.jpg)

底层引擎有三部分组成，流计算引擎包括 Flink Streaming 和 Spark Streaming，存储部分主要依靠 Druid; 流计算引擎之上，我们主要开发了 WebIDE 和任务管理功能，WebIDE 在 Web 内部集成，我们会为用户提供相应模板，用户根据相应的模板进行进行开发，即可形成自己的流计算任务，简化了一些逻辑较简单的常见 ETL、Join 任务的开发。任务完成后可通过任务管理平台直接提交，同时用户自己在本地开发的流计算任务也可以上传到平台，通过平台执行，平台对任务提供相应的指标检测。基于 Druid 引擎，配置 Druid 数据源，通过数据源衍生指标，每一个指标其实就是对 Druid 的一个查询。用户可针对指标配置告警。上图右侧的 DCube 是一个拖拽式数据分析的前端工具，DSQL 让用户可直接写 SQL 的方式输出 Druid 的即席查询能力。
　　
根据配置的指标进行告警，分为两大类，一类是阈值告警; 一类是模型告警。通常对规律性不太强的数值配置阈值告警，对规律性较强的指标配置模型告警。如滴滴每天的订单呼叫量基本上呈现一个早高峰、一个晚高峰，中间较平稳的状态。通常会选取过去一段时间的数据进行模型训练，由用户在得到的预测基线上设置置信区间。如果数据超过置信区间，就会报警。当然，也会存在一些较难处理的特殊情况，比如突然下雨、热门电影首映结束等导致的订单激增，需要额外去考虑一些情况。
![](/assets/Druid 在滴滴应用_图3-2.jpg)

上图为基本工作流，整体工作流程为由 MySQL 的 binlog 和日志采集数据，形成原始 topic，经过 ETL 或者多流 Join 进入清洗后的 topic，此时用户可以使用平台提供的魔板功能或自行开发流计算任务，我们会为所有流计算任务定制一些默认的实时指标，这些指标和用户的业务数据都会在 Druid 中建立 datasource。Druid 也可通过 Hive 离线导入数据，离线数据源和实时数据源两部分组成了 Druid 的基本数据，之后根据基本数据构建业务指标、任务指标，完成报警配置。如果业务方具备一定开发能力，需要把数据接入到自己的系统，我们也会提供一些 Open API。平台为用户提供了自助式的 Druid 数据源接入页面，极大简化地了 Druid 数据接入的复杂过程。
　　
Druid 查询采用 100% SQL 的 Web 化配置。Druid 原生查询是 DSL，类似 JSON 格式，但学习成本较高。在支持 SQL 之后，除了部分用户自建服务外，平台所有查询全部迁移到 SQL。
　　
在平台化过程中，我们遇到了一些挑战：一是核心业务与非核心业务共享资源，存在一定风险; 二是用户自助提交任务配置、查询不合理，造成异常情况，甚至影响整个集群的稳定性; 三是随着业务的快速发展，Druid 依赖的组件都需要热迁移到独立部署环境。在滴滴内部，由于 Druid 数据源基本都是用户自助接入，所以业务增长迅速，一年时间几乎涨了四倍，这对 Druid 依赖组件的热迁移提出了要求。
　　
针对不同重要程度的业务共享资源问题，首先建设 Druid 集群的异地双活机制，核心数据源的集群级双活。其次，通过统一网关对用户屏蔽多集群细节，同时根据用户身份进行查询路由，实现查询资源隔离。最后，进行业务分级，核心业务进行集群级双活，对查询资源需求较大但不过分要求实时性的业务分配独立的查询资源组，其他用户使用默认资源池。
![](/assets/Druid 在滴滴应用_图3-3.jpg)

上图为基本架构图，首先我们会有多个查询节点分布在不同集群，非核心数据源单写到公共集群，核心数据源双写到两个集群，用户使用身份验证 key 通过网关路由进行查询。
　　
针对用户配置与查询不合理造成的异常，我们主要做了以下三点：一是引擎层面进行 bad case 防范，例如，druid 数据时间字段设置合理的时间窗口限制，如果数据时间范围异常，我们就会对它进行抛弃; 二是对 Druid 原生 API 进行封装，提供更加合理的默认配置，主要针对实时任务时长、任务数量以及内存进行配置; 三是完善指标监控体系与异常定位手段，保证捕捉到异常查询。Druid 和网关日志通常会通过流计算任务进行处理，然后把它们分别写入 Druid 和 ES，数值指标会上报到 Graphite，通过 Grafana 进行展示，综合利用 Druid 的聚合分析能力与和 ES 的明细查询能力定位异常。
　　
针对依赖组件的热迁移问题，Druid 主要依赖的组件有三个：ZooKeeper、MySQL、HDFS。在过去一年，滴滴完成了三大组件的迁移，主要过程如下：
　　
1、ZooKeeper 迁移原理：扩容 - 集群分裂 - 缩容
![](/assets/Druid 在滴滴应用_图3-4.jpg)

在滴滴内部，ZK 原来是与其他业务共用的，我们需要保证其他业务和 Druid 都不停服的情况下，把 Druid 的 ZK 集群单独迁移出来，所以我们采用了上述迁移方案。核心思路就是先扩容，随后利用两套集群配置，触发集群分裂，最后缩容掉不需要的节点。如图 4 所示，这七台 ZK 配置其实有两套，第一套是 12347 五台，第二套是 567 三台，但它们的 leader 都是 ZK7，此时 7 个节点同属一个集群。当重启 ZK7 之后，两套配置的 ZK 节点会分别独立选取 leader，此时进行集群分裂变成两个单独的 ZK 集群。

2、MySQL 热迁移实践
